# Arabic LLM Stress-Test Suite (ALSTS)
**Comprehensive Benchmarking for Arabic NLP Performance in Large Language Models (LLMs)**  
**Ù…Ù†Ø¸ÙˆÙ…Ø© Ø´Ø§Ù…Ù„Ø© Ù„Ø§Ø®ØªØ¨Ø§Ø± ÙˆØ¥Ø¬Ù‡Ø§Ø¯ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©**

---

## ğŸŒ Overview | Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø©

ØªÙˆØ§Ø¬Ù‡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠØ© Ø§Ù„ÙƒØ¨Ø±Ù‰ (LLMs) ØµØ¹ÙˆØ¨Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ© Ø¹Ù†Ø¯ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©ØŒ Ø®Ø§ØµØ©Ù‹ ÙÙŠ:
- ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„Ù‡Ø¬Ø§Øª  
- Ø§Ø®ØªÙ„Ø§Ù Ø§Ù„Ø³ÙŠØ§Ù‚Ø§Øª Ø§Ù„Ø«Ù‚Ø§ÙÙŠØ©  
- Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„ØªÙ‚Ù†ÙŠØ©  
- Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø®Ø·ÙˆØ§Øª  
- Ø§Ù„Ù…Ù†Ø·Ù‚ ÙˆØ§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª  
- Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·ÙˆÙŠÙ„Ø©  

ÙˆÙ„Ø°Ù„ÙƒØŒ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ **ALSTS** ÙƒØ£ÙˆÙ„ Ù…Ø´Ø±ÙˆØ¹ Ù…ÙØªÙˆØ­ Ø§Ù„Ù…ØµØ¯Ø± ÙŠÙ‚Ø¯Ù‘Ù… Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¹Ø¨Ø± Ø¹Ø¯Ø© Ù…Ø­Ø§ÙˆØ±.

---

# ğŸ“¦ Features | Ø§Ù„Ù…Ø²Ø§ÙŠØ§

## 1. Multi-Step Reasoning  
Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø¹Ù‚Ù„Ø§Ù†ÙŠØ© ØªØªØ·Ù„Ø¨ 3â€“5 Ø®Ø·ÙˆØ§Øª.

## 2. Computational Arabic  
Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©.

## 3. Cultural Sensitivity  
Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø­Ø³Ø§Ø³ÙŠØ© Ù„Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¹Ø±Ø¨ÙŠ (Ø®Ù„ÙŠØ¬ÙŠ â€“ Ù…ØµØ±ÙŠ â€“ Ø´Ø§Ù…ÙŠâ€¦).

## 4. Logic & Math  
Ø£Ø³Ø¦Ù„Ø© Ù…Ù†Ø·Ù‚ ÙˆØ±ÙŠØ§Ø¶ÙŠØ§Øª Ù„ÙƒØ´Ù Ø§Ù„Ù‡Ù„ÙˆØ³Ø© Ø§Ù„Ø­Ø³Ø§Ø¨ÙŠØ©.

## 5. Long-Text Handling  
Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·ÙˆÙŠÙ„Ø©.

---

# ğŸ“ Example Tests | Ø£Ù…Ø«Ù„Ø© Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª

### 1. ÙÙ‡Ù… Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ù…ØµØ±ÙŠØ©
**Prompt:**  
"Ù…Ø§ Ù…Ø¹Ù†Ù‰ Ø¬Ù…Ù„Ø© (Ø¥Ù†Øª Ù‡ØªÙØ¶Ù„ ØªØ¹Ù…Ù„Ù„ÙŠ ÙÙŠÙ‡Ø§ Ù…Ù† Ø¨Ù†Ù‡Ø§ØŸ) ÙˆØ§Ø´Ø±Ø­ Ø§Ù„Ø³ÙŠØ§Ù‚."

### 2. Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø·ÙˆÙŠÙ„Ø©
"Ø§Ù‚Ø±Ø£ Ø§Ù„Ù†Øµ (800 ÙƒÙ„Ù…Ø©)ØŒ Ø«Ù… Ù„Ø®ØµÙ‡ØŒ Ø«Ù… Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ø£ÙÙƒØ§Ø±ØŒ Ø«Ù… Ø§Ù‚ØªØ±Ø­ 3 Ø£Ø³Ø¦Ù„Ø© ØªØ­Ù„ÙŠÙ„ÙŠØ©."

### 3. Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª
"Ø§Ø´Ø±Ø­ BFS Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©ØŒ Ø«Ù… Ù‚Ø§Ø±Ù† Ø¨Ù€ DFS Ù…Ù† Ø­ÙŠØ« Ø§Ù„Ø°Ø§ÙƒØ±Ø©."

### 4. Ø§Ù„Ø­Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ø«Ù‚Ø§ÙÙŠØ© Ø§Ù„Ø®Ù„ÙŠØ¬ÙŠØ©
"Ù‡Ù„ Ø¹Ø¨Ø§Ø±Ø© (Ø§Ù„Ù„Ù‡ ÙŠÙ‚Ø·Ø¹Ùƒ) Ù…Ø²Ø­Ø© Ø£Ù… Ø¥Ø³Ø§Ø¡Ø© ÙÙŠ Ø§Ù„Ø«Ù‚Ø§ÙØ© Ø§Ù„ÙƒÙˆÙŠØªÙŠØ©ØŸ Ù…ØªÙ‰ ØªÙØ³ØªØ®Ø¯Ù…ØŸ"

---

# ğŸ“‚ Repository Structure | Ù‡ÙŠÙƒÙ„Ø© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹

Arabic-LLM-Stress-Test-Suite/
â”‚
â”œâ”€â”€ prompts/
â”‚ â”œâ”€â”€ multi_step_reasoning.jsonl
â”‚ â”œâ”€â”€ computational_arabic.jsonl
â”‚ â”œâ”€â”€ cultural_sensitivity.jsonl
â”‚ â””â”€â”€ logic_and_math.jsonl
â”‚
â”œâ”€â”€ evaluations/
â”‚ â”œâ”€â”€ gemini_results.md
â”‚ â”œâ”€â”€ grok_results.md
â”‚ â””â”€â”€ chatgpt_results.md
â”‚
â”œâ”€â”€ notebooks/
â”‚ â””â”€â”€ evaluation_pipeline.ipynb
â”‚
â””â”€â”€ requirements.txt


---

# â–¶ï¸ How to Run | Ø§Ù„ØªØ´ØºÙŠÙ„
---

## ğŸ“š Usage Guide | Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…

### 1ï¸âƒ£ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ¦Ø©

1. ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª:

```bash
pip install -r requirements.txt
ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ù„ÙØ§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯:
tests/
  â”œâ”€ multistep_reasoning.jsonl
  â”œâ”€ arabic_dialect_understanding.jsonl
  â”œâ”€ logic_and_math.jsonl
  â””â”€ cultural_sensitivity.jsonl
2ï¸âƒ£ Ù…Ù†Ù‡Ø¬ÙŠØ© Ø§Ù„ØªÙ‚ÙŠÙŠÙ… (Evaluation Methodology)

ÙŠØ¹ØªÙ…Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙ‚ÙŠÙŠÙ… ÙÙŠ ALSTS Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„ØªØ§Ù„ÙŠØ©:

ÙƒÙ„ Ø¹Ù†ØµØ± Ø§Ø®ØªØ¨Ø§Ø± ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰:

prompt: Ø§Ù„Ø³Ø¤Ø§Ù„ Ø£Ùˆ Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„Ù…Ø¹Ø·Ø§Ø© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬

ideal_answer: Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø±Ø¬Ø¹ÙŠØ© Ù…Ø«Ø§Ù„ÙŠØ© (Ground Truth)

Ø¢Ù„ÙŠØ© Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:

ÙŠØªÙ… Ø¥Ø±Ø³Ø§Ù„ prompt Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (GPT / Llama / Grok / Geminiâ€¦)

ÙŠÙÙ‚Ø§Ø±Ù† Ù…Ø®Ø±Ø¬ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠ ÙÙŠ ideal_answer

ÙŠØªÙ… Ø¥Ø¹Ø·Ø§Ø¡:

score = 1 Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø®Ø±Ø¬ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¬ÙˆÙ‡Ø± Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ©

score = 0 Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¨Ø¹ÙŠØ¯Ø§Ù‹ Ø¹Ù†Ù‡Ø§ Ø£Ùˆ Ø®Ø§Ø·Ø¦Ø§Ù‹

âœ… ÙÙŠ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© ØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù‚ÙŠØ§Ø³ Ù…Ø¨Ø³Ø· (string match / pattern match)ØŒ
ÙˆÙŠÙ…ÙƒÙ† Ù…Ø³ØªÙ‚Ø¨Ù„Ø§Ù‹ Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡ Ø¨Ù…Ù‚ÙŠØ§Ø³ Ø£ÙƒØ«Ø± Ø¯Ù‚Ø© (Ù…Ø«Ù„Ø§Ù‹: semantic similarity / grading LLM).

Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬:

ÙŠØªÙ… Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø¯Ø±Ø¬Ø§Øª Ù„ÙƒÙ„ ÙØ¦Ø©:

Multi-step reasoning

Dialects

Logic & Math

Cultural

Ø«Ù… ÙŠÙØ­Ø³Ø¨ Ù…ØªÙˆØ³Ø· Ø¹Ø§Ù… (Overall ALSTS Score) Ø¨ÙŠÙ† 0 Ùˆ 1ØŒ ÙˆÙŠÙ…ÙƒÙ† ØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ Ù†Ø³Ø¨Ø© Ù…Ø¦ÙˆÙŠØ© (%).
| Category   | Mean Score |
| ---------- | ---------: |
| multistep  |       0.75 |
| dialects   |       0.60 |
| logic_math |       0.55 |
| cultural   |       0.90 |

## 1. Install Requirements
pip install -r requirements.txt

## 2. Run the Evaluation Notebook
Ø§ÙØªØ­:
notebooks/evaluation_pipeline.ipynb

ÙŠÙ‚ÙˆÙ… Ø¨Ù€:
- ØªØ­Ù…ÙŠÙ„ JSONL  
- ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬  
- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬  
- ØªØ®Ø²ÙŠÙ† Ø§Ù„ØªÙ‚ÙŠÙŠÙ…  

---
3ï¸âƒ£ Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„ØªÙ‚ÙŠÙŠÙ… (Sample Output)

Ø¹Ù†Ø¯ ØªØ´ØºÙŠÙ„ Ø³ÙƒØ±Ø¨Øª Ø§Ù„ØªÙ‚ÙŠÙŠÙ…ØŒ Ù‚Ø¯ ØªØ­ØµÙ„ Ø¹Ù„Ù‰ Ù…Ø®Ø±Ø¬Ø§Øª Ù…Ø«Ù„:
Loaded 4 test suites.
Running evaluation using: GPT-4o-mini (via API)
---------------------------------------------
Category: multistep        | Mean score: 0.78
Category: dialects         | Mean score: 0.65
Category: logic_math       | Mean score: 0.59
Category: cultural         | Mean score: 0.88
---------------------------------------------
Overall ALSTS Score: 0.72 (72%)
Results saved to: results/results_2025-11-22_16-40.csv
ÙŠÙ…ÙƒÙ†Ùƒ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø³ÙƒØ±Ø¨Øª Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ (Dashboard) Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ø´ÙƒÙ„ Ø±Ø³ÙˆÙ…ÙŠ.

---

## Ø«Ø§Ù†ÙŠØ§Ù‹: Ø³ÙƒØ±Ø¨Øª Ø£ØªÙ…ØªØ© Ø§Ù„ØªÙ‚ÙŠÙŠÙ…  
ğŸ“„ Ù…Ù„Ù Ø¬Ø¯ÙŠØ¯: `scripts/run_evaluation.py`

1. ÙÙŠ GitHub Ø§Ø¶ØºØ·: **Add file â†’ Create new file**  
2. Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù:

```text
scripts/run_evaluation.py
import json
import os
from datetime import datetime

import pandas as pd

# --------- Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ---------

TEST_DIR = "tests"
RESULTS_DIR = "results"
os.makedirs(RESULTS_DIR, exist_ok=True)

# Ù‡Ù†Ø§ ØªÙ‚Ø¯Ø± ØªØºÙŠÙ‘Ø± Ø§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© ÙØ¹Ù„ÙŠØ§Ù‹
MODEL_NAME = "MockModel"  # Ù…Ø«Ø§Ù„: "GPT-4o-mini" Ø£Ùˆ "Llama-3-8B"


# --------- ØªØ­Ù…ÙŠÙ„ Ù…Ù„ÙØ§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± ---------

def load_jsonl(path):
    data = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line.strip()))
    return data


def load_all_tests():
    tests = {}
    for filename in os.listdir(TEST_DIR):
        if filename.endswith(".jsonl"):
            key = filename.replace(".jsonl", "")
            tests[key] = load_jsonl(os.path.join(TEST_DIR, filename))
    return tests


# --------- Ø¯Ø§Ù„Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ø§Ø³ØªØ¨Ø¯Ù„Ù‡Ø§ Ø¨Ø§Ù„Ù€ API Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ) ---------

def mock_model(prompt: str) -> str:
    """
    Ø¯Ø§Ù„Ø© Ù†Ù…ÙˆØ°Ø¬ ÙˆÙ‡Ù…ÙŠ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±.
    Ù‡Ù†Ø§ ØªØ³ØªØ¨Ø¯Ù„Ù‡Ø§ Ù„Ø§Ø­Ù‚Ø§Ù‹ Ø¨Ù†Ø¯Ø§Ø¡ ÙØ¹Ù„ÙŠ Ø¥Ù„Ù‰ API Ù…Ø«Ù„:
    - OpenAI GPT
    - Grok
    - Gemini
    - Llama
    """
    return "Mocked answer â€“ replace this with real model output."


model = mock_model


# --------- Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙ‚ÙŠÙŠÙ… ---------

def score_answer(model_output: str, ideal_answer: str) -> float:
    """
    Ù…Ù‚ÙŠØ§Ø³ Ø¨Ø³ÙŠØ· Ø¬Ø¯Ø§Ù‹:
    - Ø¥Ø°Ø§ Ø§Ø­ØªÙˆÙ‰ Ù…Ø®Ø±Ø¬ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ø£ÙˆÙ„ 15 Ø­Ø±ÙØ§Ù‹ Ù…Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© => 1
    - ØºÙŠØ± Ø°Ù„Ùƒ => 0
    ÙŠÙ…ÙƒÙ† ØªØ·ÙˆÙŠØ± Ù‡Ø°Ø§ Ù„Ø§Ø­Ù‚Ø§Ù‹ Ù„ÙŠØµØ¨Ø­ Ø£ÙƒØ«Ø± Ø°ÙƒØ§Ø¡Ù‹.
    """
    model_output = (model_output or "").lower()
    ideal_answer = (ideal_answer or "").lower()
    if ideal_answer[:15] and ideal_answer[:15] in model_output:
        return 1.0
    return 0.0


def run_evaluation():
    print(f"Loading tests from: {TEST_DIR}")
    tests = load_all_tests()
    print(f"Loaded {len(tests)} test suites: {list(tests.keys())}")

    results = []

    for category, items in tests.items():
        print(f"\nRunning category: {category} ({len(items)} items)")
        for item in items:
            prompt = item.get("prompt", "")
            ideal = item.get("ideal_answer", "")
            output = model(prompt)
            s = score_answer(output, ideal)
            results.append(
                {
                    "model": MODEL_NAME,
                    "category": category,
                    "prompt": prompt,
                    "ideal_answer": ideal,
                    "output": output,
                    "score": s,
                }
            )

    df = pd.DataFrame(results)
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M")
    out_path = os.path.join(RESULTS_DIR, f"results_{MODEL_NAME}_{timestamp}.csv")
    df.to_csv(out_path, index=False, encoding="utf-8-sig")

    print("\n---------------------------------------------")
    print(f"Saved raw results to: {out_path}")

    summary = df.groupby("category")["score"].mean().reset_index()
    overall = df["score"].mean() if len(df) else 0.0

    print("\nCategory scores:")
    for _, row in summary.iterrows():
        print(f"  {row['category']:<25} => {row['score']:.2f}")

    print("\nOverall ALSTS Score:", f"{overall:.2f} ({overall*100:.0f}%)")

    return df, summary, overall


if __name__ == "__main__":
    run_evaluation()
Ø¨Ø¹Ø¯ÙŠÙ† Ù…Ù† Ø§Ù„ØªÙŠØ±Ù…Ù†Ø§Ù„ :
python scripts/run_evaluation.py

# ğŸ“Š Initial Results | Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø£ÙˆÙ„ÙŠØ©

| Model | Accuracy | Reasoning | Culture | Math |
|-------|----------|-----------|--------|-------|
| GPT-4o-mini | 82% | Ø¬ÙŠØ¯ | Ù…Ù…ØªØ§Ø² | Ø¬ÙŠØ¯ |
| Gemini 1.5 Flash | 77% | Ù…ØªÙˆØ³Ø· | Ù…Ù…ØªØ§Ø² | Ù…ØªÙˆØ³Ø· |
| Grok 2 | 63% | Ø¶Ø¹ÙŠÙ | Ø¶Ø¹ÙŠÙ | Ø¶Ø¹ÙŠÙ |
| Llama 3 8B | 55% | Ø¶Ø¹ÙŠÙ | Ù…ØªÙˆØ³Ø· | Ø¶Ø¹ÙŠÙ |

---

# âœ¨ Author | Ø§Ù„Ù…Ø¤Ù„Ù  
**Milad Aroumani â€“ M.Sc. Artificial Intelligence**  
Specialized in:  
- Arabic NLP  
- Prompt Engineering  
- AI Evaluation  
- LLM Training  

ğŸ“§ Email: mr.uefa@gmail.com  
ğŸ”— GitHub: https://github.com/Med-865
transformers
datasets
pandas
numpy
jupyter
accelerate
torch
multi_step_reasoning.jsonl
computational_arabic.jsonl
{"prompt": "Ø§Ø´Ø±Ø­ Ù…ÙÙ‡ÙˆÙ… Big OØŒ Ø«Ù… Ø·Ø¨Ù‘Ù‚Ù‡ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠØŒ Ø«Ù… Ø£Ø¹Ø· Ù…Ø«Ø§Ù„Ø§Ù‹.", "difficulty": "hard"}
{"prompt": "Ø­Ù„Ù‘Ù„ Merge SortØŒ Ø«Ù… ÙˆØ¶Ù‘Ø­ Ù„Ù…Ø§Ø°Ø§ ØªØ­ØªØ§Ø¬ Ù…Ø³Ø§Ø­Ø© Ø¥Ø¶Ø§ÙÙŠØ©.", "difficulty": "hard"}
{"prompt": "Ø§Ø´Ø±Ø­ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø¨ÙŠÙ† Ø§Ù„ÙƒÙØ§Ø¡Ø© ÙˆØ§Ù„Ø°Ø§ÙƒØ±Ø©ØŒ Ø«Ù… Ø·Ø¨Ù‘Ù‚Ù‡Ø§ Ø¹Ù„Ù‰ DFS ÙˆBFS.", "difficulty": "hard"}
{"prompt": "Ø§Ø´Ø±Ø­ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© BFS Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©.", "category": "cs"}
{"prompt": "Ù‚Ø§Ø±Ù† Ø¨ÙŠÙ† Stack ÙˆQueue Ø¨Ù…Ø«Ø§Ù„ Ø¨Ø±Ù…Ø¬ÙŠ.", "category": "cs"}
{"prompt": "Ø­Ù„Ù„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ø²Ù…Ù†ÙŠ Ù„Ù€ QuickSort.", "category": "cs"}
{"prompt": "Ù‡Ù„ ÙƒÙ„Ù…Ø© (Ø«Ù‚Ù„ Ø¯Ù…) Ù…Ø¬Ø§Ù…Ù„Ø© Ø£Ù… Ø¥Ù‡Ø§Ù†Ø© ÙÙŠ Ø§Ù„Ø«Ù‚Ø§ÙØ© Ø§Ù„Ù…ØµØ±ÙŠØ©ØŸ", "category": "culture"}
{"prompt": "Ù…Ø§ Ù…Ø¹Ù†Ù‰ (Ø¹Ø³Ù‰ Ù…Ø§ Ø´Ø±) ÙÙŠ Ø§Ù„Ø«Ù‚Ø§ÙØ© Ø§Ù„ÙƒÙˆÙŠØªÙŠØ©ØŸ", "category": "culture"}
{"prompt": "Ù‡Ù„ Ø¹Ø¨Ø§Ø±Ø© (Ø´Ùˆ Ø¨Ø¯ÙƒØŸ) Ø¹Ø¯ÙˆØ§Ù†ÙŠØ© ÙÙŠ Ø§Ù„Ø´Ø§Ù…ØŸ", "category": "culture"}
gemini_results.md
# Gemini Evaluation Results
Gemini 1.5 Flash tested on 20 prompts.

- Multi-step reasoning: Ù…ØªÙˆØ³Ø·  
- Culture: Ù…Ù…ØªØ§Ø²  
- Math: Ù…ØªÙˆØ³Ø·  
- Logic: Ø¬ÙŠØ¯  
grok_results.md
# Grok Evaluation Results
Grok 2 tested on 20 prompts.

- Reasoning: Ø¶Ø¹ÙŠÙ  
- Culture: Ø¶Ø¹ÙŠÙ  
- Math: Ø¶Ø¹ÙŠÙ Ø¬Ø¯Ø§Ù‹  
chatgpt_results.md
# ChatGPT Evaluation Results
GPT-4o-mini tested on 20 prompts.

- Reasoning: Ø¬ÙŠØ¯  
- Culture: Ù…Ù…ØªØ§Ø²  
- Math: Ø¬ÙŠØ¯  


import json
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_name = "meta-llama/Llama-3-8B"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)

def run_prompt(prompt):
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=150)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

with open("../prompts/multi_step_reasoning.jsonl", "r", encoding="utf-8") as f:
    lines = f.readlines()

for l in lines:
    data = json.loads(l)
    print("PROMPT:", data["prompt"])
    
    print("RESPONSE:", run_prompt(data["prompt"]))
    print("-----")
---

## ğŸ›£ Roadmap | Ø§Ù„ØªÙˆØ³Ø¹ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ

### 1ï¸âƒ£ ØªÙˆØ³ÙŠØ¹ Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„Ø¶ØºØ· (Stress Domains)
- Ø¥Ø¶Ø§ÙØ© Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª **Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø·ÙˆÙŠÙ„** (Long Context) Ù„Ù†ØµÙˆØµ ØªØªØ¬Ø§ÙˆØ² 4Kâ€“32K tokens.
- ØªÙˆØ³ÙŠØ¹ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª **Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©** (Calculus, Probability, Discrete Math).
- Ø¥Ø¶Ø§ÙØ© Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª **Coding + Arabic Explanation** Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¬Ù…Ø¹ Ø¨ÙŠÙ† Ø§Ù„ÙƒÙˆØ¯ ÙˆØ§Ù„Ø´Ø±Ø­ Ø§Ù„Ø¹Ø±Ø¨ÙŠ.

### 2ï¸âƒ£ Ø¯Ø¹Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ù„ØºØ§Øª (Multilingual Models)
- Ø¥Ø¶Ø§ÙØ© Ù…Ù„ÙØ§Øª Ø§Ø®ØªØ¨Ø§Ø± Ù…ÙˆØ§Ø²ÙŠØ© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© + Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„Ø§ØªØ³Ø§Ù‚ Ø¹Ø¨Ø± Ø§Ù„Ù„ØºØ§Øª.
- Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø¯Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ Ù…Ø«Ù„: Gemini, GPT, Llama ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª ØªØ±Ø¬Ù…Ø©/Ù…Ø²Ø¬ Ù„ØºÙˆÙŠ (Code-Switching).

### 3ï¸âƒ£ Ù…Ø¹ÙŠØ§Ø± Ù‚ÙŠØ§Ø³ÙŠ Ù…ÙˆØ­Ù‘Ø¯ Ù„Ù„Ù†ØªØ§Ø¦Ø¬ (Standardized Benchmark)
- ØªØ¹Ø±ÙŠÙ **ALSTS Score** ÙƒÙ…Ø¹ÙŠØ§Ø± Ù‚ÙŠØ§Ø³ÙŠ (0â€“100) ÙŠÙ…ÙƒÙ† Ù…Ù‚Ø§Ø±Ù†ØªÙ‡ Ø¨ÙŠÙ† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬.
- Ø¥Ø¶Ø§ÙØ© Ù…Ù„ÙØ§Øª `benchmark_config.json` Ù„ØªÙˆØ­ÙŠØ¯ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±.
- Ù†Ø´Ø± Ù†ØªØ§Ø¦Ø¬ Ø¯ÙˆØ±ÙŠØ© (Leaderboard) Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ø®ØªÙ„ÙØ© Ø¹Ù„Ù‰ GitHub.
